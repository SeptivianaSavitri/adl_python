{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "ADL_Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxR5cncXs4ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymgrXMZus4iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class nn(object):\n",
        "    def __init__(self, layer):\n",
        "        self.size = layer\n",
        "        self.n = len(layer)\n",
        "        self.hl = self.n - 2\n",
        "        self.activation_func = \"sigm\"\n",
        "        self.learningRate = 0.01\n",
        "        self.momentum = 0.95\n",
        "        self.outputConnect = 1\n",
        "        self.output = \"softmax\"\n",
        "        \n",
        "        # initiate weights and weight momentum for hidden layer\n",
        "        self.W = {}\n",
        "        self.vW = {}\n",
        "        self.dW = {}\n",
        "        self.c = {}\n",
        "        #self.W = self.vW  = self.dW= self.c = {} \n",
        "        for i in range(1, self.n-1):\n",
        "            self.W[i] = np.random.normal(0, np.sqrt(2/(self.size[i-1]+1)), size=(self.size[i],self.size[i - 1]+1))\n",
        "            self.vW[i] = np.zeros(self.W[i].shape)\n",
        "            self.dW[i] = np.zeros(self.W[i].shape)\n",
        "            self.c[i] =  np.random.normal(0, np.sqrt(2/(self.size[i-1]+1)), size=(self.size[i - 1],1))\n",
        "        \n",
        "        #initiate weights and weight momentum for output layer\n",
        "        self.Ws = {}\n",
        "        self.vWs = {}\n",
        "        self.dWs =  {}\n",
        "        self.beta = {}\n",
        "        self.betaOld = {}\n",
        "        self.p= {}  \n",
        "        if self.outputConnect == 1:\n",
        "            for i in range(1,(self.hl + 1)):\n",
        "                self.Ws[i] = np.random.normal(0, np.sqrt(2/((self.W[i].shape[0])+1)), size=(self.size[len(self.size)-1],self.size[i]+1))\n",
        "                self.vWs[i] = np.zeros(self.Ws[i].shape)\n",
        "                self.dWs[i] = np.zeros(self.Ws[i].shape)         \n",
        "                self.beta[i] = 1\n",
        "                self.betaOld[i] = 1\n",
        "                self.p[i]= 1 \n",
        "        else:\n",
        "            i = 1\n",
        "            self.Ws[i] = np.random.normal(0, np.sqrt(2/((self.W[i-1].shape[0])+1)), size=(self.size[len(self.size)-1],self.size[len(self.size)-2]+1))\n",
        "            self.vWs[i] = np.zeros(self.Ws[i].shape)\n",
        "            self.dWs[i] = np.zeros(self.Ws[i].shape) \n",
        "            \n",
        "        #initiate later used variable\n",
        "        self.a = {}\n",
        "        self.aas = {}\n",
        "        self.e = {}\n",
        "        self.L = {}\n",
        "        self.classlabel = {}\n",
        "        self.nop = {}\n",
        "        self.nodes = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcaXYtrms4iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parameter(object):\n",
        "    def __init__(self, nn,layer,K):\n",
        "        self.nn = nn\n",
        "        self.ev = {}\n",
        "        self.size = layer\n",
        "        self.prune_list       = 0;\n",
        "        self.prune_list_index = [];\n",
        "        self.ev[1] = {'layer': layer, 'kp':0, 'kl':0 ,'miu_x_old':0, 'var_x_old':0, 'kl':0,'K':K, 'cr':0,'node':{},\n",
        "                     'BIAS2':{}, 'VAR':{}, 'miu_NS_old':0, 'var_NS_old':0, 'miu_NHS_old':0, 'var_NHS_old':0,\n",
        "                     'miumin_NS':[], 'miumin_NHS':[], 'stdmin_NS':[], 'stdmin_NHS':[]}\n",
        "        self.Loss = {}\n",
        "        self.cr = {}\n",
        "        self.wl = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCnP4Bo2s4i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Performance(object):\n",
        "    def __init__(self):\n",
        "        self.update_time = 0\n",
        "        self.ev = {}\n",
        "        self.test_time = 0\n",
        "        self.classification_time    = 0;\n",
        "        self.layer = 0;\n",
        "        self.ev[1] = {'f_measure': 0, 'g_mean':0, 'recall':0, 'precision':0}\n",
        "        self.LayerWeight = 0\n",
        "        self.meanode = []\n",
        "        self.stdnode = []\n",
        "        self.NumberOfParameters = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlVmBVMws4i2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_array(x):                                        \n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu_array(x):\n",
        "    return abs(x) * (x > 0)\n",
        "\n",
        "def softmax_array(x):\n",
        "    list_re = []\n",
        "    for arr in x:\n",
        "        e_x = np.exp(arr - np.max(arr))\n",
        "        result = e_x / e_x.sum()\n",
        "        list_re.append(result)\n",
        "    return np.array(list_re)  \n",
        "\n",
        "def argmax_array(x):\n",
        "    list_re = []\n",
        "    for arr in x:\n",
        "        result = [arr.argmax()]\n",
        "        list_re.append(result)\n",
        "    return np.array(list_re)  \n",
        "\n",
        "def valuemax_array(x):\n",
        "    list_re = []\n",
        "    for arr in x:\n",
        "        result = [arr.max()]\n",
        "        list_re.append(result)\n",
        "    return np.array(list_re)  \n",
        "\n",
        "def update_beta(x):\n",
        "    result = {}\n",
        "    sum_val = sum(x.values())\n",
        "#     if(all(value == 0 for value in x.values())):\n",
        "#         return x\n",
        "   \n",
        "    for k, v in x.items():\n",
        "        result[k] = v/sum_val        \n",
        "    return result\n",
        "\n",
        "def checkbeta(dict1):\n",
        "    count = 0\n",
        "    for i in dict1.values():\n",
        "        if(i != 0):\n",
        "            count += 1\n",
        "    return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B71ghWas4i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nettestparallel(nn, x, y, ev):\n",
        "    nop = []\n",
        "    #feedforward\n",
        "    nn = netfeedforward(nn,x,y)\n",
        "    (m1,m2) = y.shape\n",
        "    compare0 = 0\n",
        "    comparenot0 = 0\n",
        "    #obtain true class\n",
        "    act = argmax_array(y)\n",
        "    for i in range(1,nn.hl+1):\n",
        "        nn.classlabel[i] = {}\n",
        "    if(len(nn.betaOld)>1):\n",
        "        nn.sigma = np.zeros((m1,m2))\n",
        "        for t in range(0,m1):\n",
        "            for i in range(1,nn.hl+1):\n",
        "                if(nn.beta[i] != 0):\n",
        "                    #obtain predicted label, note : layer weight betaOld is fixed\n",
        "                    nn.sigma[t] = nn.sigma[t] + (nn.aas[i][t] * nn.betaOld[i])               \n",
        "                    nn.classlabel[i][t] = nn.aas[i][t].argmax()\n",
        "                    compare = act[t][0] - nn.classlabel[i][t]\n",
        "                    #train the weighted voting\n",
        "                    if(compare != 0):\n",
        "                        nn.beta[i] = max(nn.beta[i] * nn.p[i], 0)\n",
        "                        nn.p[i] = max(nn.p[i] - 0.01 , 0)\n",
        "                    elif(compare == 0):\n",
        "                        nn.beta[i] = min(nn.beta[i] * (1+nn.p[i]), 1)\n",
        "                        nn.p[i] = min(nn.p[i] + 0.01 , 1)\n",
        "\n",
        "                #last element on data chunk        \n",
        "                if(t==m1):\n",
        "                    #calculate number of parameter\n",
        "                    if(nn.beta[i] != 0):\n",
        "                        (c,d) = nn.Ws[i].shape\n",
        "                        vw = 1\n",
        "                    else:\n",
        "                        c = 0\n",
        "                        d = 0\n",
        "                        vw = 0\n",
        "                    (a,b) = nn.W[i].shape\n",
        "                    nop.append(a*b + c*d + vw)\n",
        "                    \n",
        "                    #calculate the number of node in each hidden layer\n",
        "                    nn.nodes[i][nn.t] = ev[1]['K']\n",
        "            #print(nn.beta)\n",
        "            nn.beta = update_beta(nn.beta);\n",
        "            #print(nn.beta)\n",
        "    elif(len(nn.betaOld)==1):\n",
        "        nn.sigma = nn.aas[1]\n",
        "        (c,d) = nn.Ws[1].shape\n",
        "        vw = 1\n",
        "        (a,b) = nn.W[1].shape\n",
        "        nop = a*b + c*d + vw\n",
        "        nn.nodes[1] = {nn.t : ev[1]['K']}\n",
        "    \n",
        "    nn.nop[nn.t] = np.sum(nop)\n",
        "\n",
        "    nn.mnop = [np.array(list(nn.nop.values())).mean(),np.array(list(nn.nop.values())).std()]\n",
        "    \n",
        "    #update voting weight\n",
        "    nn.betaOld = nn.beta.copy()\n",
        "    nn.index = max(nn.beta, key=nn.beta.get)\n",
        "    \n",
        "    \n",
        "    #calculate classification rate\n",
        "    [raw_out, out] = [valuemax_array(nn.sigma), argmax_array(nn.sigma)]\n",
        "    nn.bad = (np.where(out != act)[0]).reshape(-1,1)\n",
        "    nn.cr = 1 - (nn.bad.shape[0]/m1)\n",
        "    nn.residual_error = 1 - raw_out\n",
        "    nn.out = out\n",
        "    nn.act = act\n",
        "    return [nn]\n",
        "    #return nn,out,act,raw_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8O7QNWts4i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def netfeedforward(nn,x,y):\n",
        "    n = nn.n\n",
        "    m = x.shape[0]\n",
        "    ones = np.ones(shape= (m,1))\n",
        "    x = np.append(ones, x, axis=1 ) #append columns of 1 in matrix for bias\n",
        "    nn.a[1] = x\n",
        "    #feedforward from input layer through all the hidden layer\n",
        "    for i in range(2,n):\n",
        "        if(nn.activation_func == 'sigm'):\n",
        "            nn.a[i] = sigmoid_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
        "        elif(nn.activation_func == 'relu'):\n",
        "            nn.a[i] = relu_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
        "        ones = np.ones(shape= (m,1))\n",
        "        nn.a[i] = np.append(ones, nn.a[i], axis=1 )\n",
        "    \n",
        "    #propagate to the output layer\n",
        "    for i in range(1,nn.hl+1):\n",
        "        if nn.beta[i] != 0:\n",
        "            if(nn.output == 'sigm'):\n",
        "                nn.aas[i] = sigmoid_array(np.matmul(nn.a[i+1], nn.Ws[i].T))\n",
        "            elif(nn.output == 'linear'):\n",
        "                nn.aas[i] = np.matmul(nn.a[i+1], nn.Ws[i].T)   \n",
        "            elif(nn.output == 'softmax'):\n",
        "                nn.aas[i] = softmax_array(np.matmul(nn.a[i+1],nn.Ws[i].T)) \n",
        "            \n",
        "            \n",
        "            #calculate error\n",
        "            nn.e[i] = y - nn.aas[i]\n",
        "               \n",
        "            #calculate loss function\n",
        "            if(nn.output == 'sigm' or nn.output == 'linear'):\n",
        "                nn.L[i] = 0.5 * (np.sum(np.sum(nn.e[i]**2))) / m\n",
        "            elif(nn.output == 'softmax'):\n",
        "                nn.L[i] = -np.sum(np.sum(y * np.log(nn.aas[i]))) / m\n",
        "        \n",
        "            #print(nn.L) \n",
        "    return nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP3zRYaVs4i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#net configuration while training\n",
        "def netconfigtrain(layer):\n",
        "    net = nn(layer)\n",
        "    net.layer = layer\n",
        "    net.n = len(layer)\n",
        "    net.activation_func = \"sigm\"\n",
        "    net.learningRate = 0.01\n",
        "    net.momentum = 0.95\n",
        "    net.output = \"softmax\"\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4oMtzIhs4i9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate probit function\n",
        "def probit(miu,std):\n",
        "    p = (miu / (1+ np.pi * (std**2)/8) ** 0.5)\n",
        "    return p\n",
        "\n",
        "#calculate recursive mean and standard deviation\n",
        "def meanstditer(miu_old, var_old,x,k):\n",
        "    miu = miu_old + (x - miu_old) / k\n",
        "    var = var_old + (x - miu_old) * (x-miu)\n",
        "    std = np.sqrt(var/k)\n",
        "  \n",
        "    if(len(miu.shape) != 2):\n",
        "        #print('len !=2')\n",
        "        #print([np.array([miu]),np.array([std]),np.array([var])])\n",
        "        return [np.array([miu]),np.array([std]),np.array([var])]\n",
        "    else:\n",
        "        return [miu,std,var]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qfrerBGs4i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def netffhl(nn, x):\n",
        "    n = nn.n-1\n",
        "    m = x.shape[0]\n",
        "    ones = np.ones(shape= (m,1))\n",
        "    x = np.append(ones, x, axis=1 ) #append columns of 1 in matrix for bias\n",
        "    nn.a[1] = x\n",
        "    #feedforward from input layer through all the hidden layer\n",
        "    last = 0\n",
        "    for i in range(2,n):\n",
        "        last = i\n",
        "        if(nn.activation_func == 'sigm'):\n",
        "            nn.a[i] = sigmoid_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
        "        elif(nn.activation_func == 'relu'):\n",
        "            nn.a[i] = relu_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
        "        ones = np.ones(shape= (m,1))\n",
        "        nn.a[i] = np.append(ones, nn.a[i], axis=1 )\n",
        "    y = nn.a[last]\n",
        "    return y\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx-7URGDs4jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def netffsingle(nn,x,y):\n",
        "    x = np.array([x])\n",
        "    y = np.array([y])\n",
        "    n = nn.n\n",
        "    m = x.shape[0]\n",
        "    nn.a[1] = x\n",
        "   \n",
        "    #feedforward form input layer through all the hidden layer \n",
        "    for i in range(2,n):\n",
        "        if(nn.activation_func == 'sigm'):\n",
        "            nn.a[i] = sigmoid_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
        "        elif(nn.activation_func == 'relu'):\n",
        "            nn.a[i] = relu_array(np.matmul(nn.a[i-1],nn.W[i-1].T))\n",
        "        ones = np.ones(shape= (m,1))\n",
        "        nn.a[i] = np.append(ones, nn.a[i], axis=1 )\n",
        "  \n",
        "    #propagate to the output layer\n",
        "    if(nn.output == 'sigm'):\n",
        "        nn.a[n] = sigmoid_array(np.matmul(nn.a[n-1],nn.W[n-1].T))\n",
        "    elif(nn.output == 'linear'):\n",
        "        nn.a[n] = np.matmul(nn.a[n-1],nn.W[n-1].T)   \n",
        "    elif(nn.output == 'softmax'):\n",
        "        nn.a[n] = softmax_array(np.matmul(nn.a[n-1], nn.W[n-1].T)) \n",
        "            \n",
        "    #calculate error\n",
        "    nn.e[1] = y - nn.a[n]\n",
        "    return nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yhudtLBs4jE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate backpropagation\n",
        "def netbackpropagation(nn):\n",
        "    n = nn.n\n",
        "    d = {}\n",
        "    if(nn.output == 'sigm'):\n",
        "        d[n] = - nn.e[1] * (nn.a[n] * (1 - nn.a[n]))      \n",
        "    elif(nn.output == 'linear' or nn.output == 'softmax'):\n",
        "        d[n] = -1 * nn.e[1]         \n",
        "    for i in range(n-1, 1, -1):\n",
        "        if(nn.activation_func == 'sigm'):\n",
        "            d_act = nn.a[i] * (1 - nn.a[i])\n",
        "        elif(nn.activation_func == 'tanh_opt'):\n",
        "            d_act = 1.7159 * 2/3 * (1 - 1 / (1.7159**2) * (nn.a[i]**2))   \n",
        "        elif(nn.activation_func == 'relu'):\n",
        "            d_act = np.zeros((1,len(nn.a[i])))\n",
        "            for i in range(len(d_act)):\n",
        "                if(nn.a[i + 1]>0):\n",
        "                    d_act[i] = 0             \n",
        "        if(i+1 == n):\n",
        "            d[i] = (np.matmul(d[i+1], nn.W[i]) * d_act)\n",
        "        else:\n",
        "            d[i] = (np.matmul(d[i+1][1:], nn.W[i]) * d_act)\n",
        "\n",
        "    for i in range(1, n): \n",
        "        if(i+1 == n):\n",
        "            nn.dW[i] = np.matmul(d[i+1].T, nn.a[i]) \n",
        "        else: \n",
        "            nn.dW[i] = np.matmul(d[i+1][:,1:].T, nn.a[i])   \n",
        "    return nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgSbPErqs4jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#update the weight\n",
        "def netupdate(nn):\n",
        "    for i in range(1, nn.n):\n",
        "        dW = nn.dW[i]\n",
        "        dW = nn.learningRate * dW       \n",
        "        if(nn.momentum > 0):\n",
        "            nn.vW[i] = nn.momentum * nn.vW[i] + dW\n",
        "            dW = nn.vW[i]\n",
        "        nn.W[i] = nn.W[i] - dW\n",
        "    return nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pEx1A6Zs4jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nettrainsingle(parameter,x,y):\n",
        "    bb = parameter.nn.W[parameter.nn.hl].shape[1]\n",
        "    grow = 0\n",
        "    prune=0\n",
        "    \n",
        "    #initialize performance matrix\n",
        "    ly          = parameter.nn.hl\n",
        "    kp          = parameter.ev[1]['kp']\n",
        "    miu_x_old   = parameter.ev[1]['miu_x_old']\n",
        "    var_x_old   = parameter.ev[1]['var_x_old']\n",
        "    kl          = parameter.ev[ly]['kl']\n",
        "    K           = parameter.ev[ly]['K']\n",
        "    node        = parameter.ev[ly]['node']\n",
        "    BIAS2       = parameter.ev[ly]['BIAS2']\n",
        "    VAR         = parameter.ev[ly]['VAR']\n",
        "    miu_NS_old  = parameter.ev[ly]['miu_NS_old']\n",
        "    var_NS_old  = parameter.ev[ly]['var_NS_old']\n",
        "    miu_NHS_old = parameter.ev[ly]['miu_NHS_old']\n",
        "    var_NHS_old = parameter.ev[ly]['var_NHS_old']\n",
        "    miumin_NS   = parameter.ev[ly]['miumin_NS']\n",
        "    miumin_NHS  = parameter.ev[ly]['miumin_NHS']\n",
        "    stdmin_NS   = parameter.ev[ly]['stdmin_NS']\n",
        "    stdmin_NHS  = parameter.ev[ly]['stdmin_NHS']\n",
        "    \n",
        "    net = netconfigtrain([1,1,1])\n",
        "    #substitute the weight to be trained to training model\n",
        "    net.activation_func = parameter.nn.activation_func\n",
        "    net.W[1]  = parameter.nn.W[ly]\n",
        "    net.vW[1] = parameter.nn.vW[ly]\n",
        "    net.dW[1] = parameter.nn.dW[ly]\n",
        "    net.W[2]  = parameter.nn.Ws[ly]\n",
        "    net.vW[2] = parameter.nn.vWs[ly]\n",
        "    net.dW[2] = parameter.nn.dWs[ly]\n",
        "   \n",
        "    #load data in shuffled\n",
        "    (N,I) = x.shape\n",
        "    kk = np.random.permutation(N)\n",
        "    ones = np.ones(shape= (N,1))\n",
        "    x = x[kk]\n",
        "    x = np.append(ones, x, axis=1 )\n",
        "    y = y[kk]\n",
        "    \n",
        "    #xavier initialization\n",
        "    n_in = parameter.ev[ly-1]['K']\n",
        "    miuNS       = np.zeros((N,1))\n",
        "    miuminNS       = np.zeros((N,1))\n",
        "    miuNHS       = np.zeros((N,1))\n",
        "    miuminNHS       = np.zeros((N,1))\n",
        "    \n",
        "    #main loop, train the model\n",
        "    for k in range(1, N+1):\n",
        "        kp = kp+1\n",
        "        kl = kl+1\n",
        "        #incremental calculation of x_tail mean and variance\n",
        "        if(k <= parameter.nn.a[1].shape[0]):\n",
        "            [miu_x, std_x, var_x] = meanstditer(miu_x_old,var_x_old,parameter.nn.a[1][k-1],kp)\n",
        "            miu_x_old = miu_x\n",
        "            var_x_old = var_x\n",
        "            \n",
        "            #expectation of z               \n",
        "            py = probit(miu_x,std_x).T\n",
        "\n",
        "            for ii in range(1, parameter.nn.hl+1):\n",
        "                if(ii == parameter.nn.hl):\n",
        "                    py = sigmoid_array(np.matmul(net.W[1],py))\n",
        "                else:\n",
        "                    py = sigmoid_array(np.matmul(parameter.nn.W[ii],py))\n",
        "                py = np.append(np.array([[1]]),py, axis = 0)\n",
        "                if(ii == 1):\n",
        "                    Ey2 = py**2\n",
        "            Ey = py\n",
        "            Ez = np.matmul(net.W[2], Ey) \n",
        "            Ez = softmax_array(Ez.T).T\n",
        "            \n",
        "            if(parameter.nn.hl>1):\n",
        "                py = Ey2\n",
        "                for ii in range(2, parameter.nn.hl+1):\n",
        "                    if(ii==parameter.nn.hl):\n",
        "                        py = sigmoid_array(np.matmul(net.W[1],py))\n",
        "                    else:\n",
        "                        py = sigmoid_array(np.matmul(parameter.nn.W[ii],py))\n",
        "                    py = np.append(np.array([[1]]),py, axis = 0)\n",
        "                Ey2 = py\n",
        "            Ez2 = np.matmul(net.W[2], Ey2) \n",
        "            Ez2 = softmax_array(Ez2.T).T\n",
        "            \n",
        "            #Network mean calculation  \n",
        "            bias2 = (Ez - np.array([[1,0]]).T)**2\n",
        "            ns = bias2\n",
        "            NS = np.linalg.norm(ns, 'fro')\n",
        "            \n",
        "            #Incremental calculation of NS mean and variance\n",
        "            [miu_NS,std_NS,var_NS] = meanstditer(miu_NS_old,var_NS_old,NS,kp)\n",
        "            miu_NS_old = miu_NS\n",
        "            var_NS_old = var_NS\n",
        "            miustd_NS  = miu_NS + std_NS\n",
        "            miuNS[k-1] = miu_NS\n",
        "\n",
        "            if(kl <= 1 or grow ==1):\n",
        "                miumin_NS = miu_NS\n",
        "                stdmin_NS = std_NS\n",
        "            else:\n",
        "                if(miu_NS < miumin_NS):\n",
        "                    miumin_NS = miu_NS\n",
        "                    stdmin_NS = std_NS\n",
        "                if(std_NS < stdmin_NS):\n",
        "                    stdmin_NS = std_NS\n",
        "            miuminNS = miumin_NS\n",
        "            miustdmin_NS  = miumin_NS + (1.3*np.exp(-NS)+0.7)*stdmin_NS\n",
        "            BIAS2[kl] = miu_NS\n",
        "            \n",
        "            #growing hidden unit\n",
        "            if(miustd_NS >= miustdmin_NS and kl>1):\n",
        "                grow = 1\n",
        "                K=K+1\n",
        "                print('The new node no {} is FORMED around sample {}'.format(K, k))\n",
        "                node[kp] = K\n",
        "                W_app = np.random.normal(0, np.sqrt(2/(n_in+1)), size=(1, bb))\n",
        "                net.W[1] = np.append(net.W[1],W_app, axis = 0)          \n",
        "                net.vW[1] = np.append(net.vW[1],np.zeros((1,bb)), axis = 0)\n",
        "                net.dW[1] = np.append(net.dW[1],np.zeros((1,bb)), axis = 0)\n",
        "\n",
        "                W2_app = np.random.normal(0, np.sqrt(2/(K+1)), size=(parameter.nn.size[-1], 1))\n",
        "                net.W[2] = np.append(net.W[2],W2_app, axis = 1)\n",
        "                net.vW[2] = np.append(net.vW[2],np.zeros((parameter.nn.size[-1], 1)), axis = 1)\n",
        "                net.dW[2] = np.append(net.dW[2],np.zeros((parameter.nn.size[-1], 1)), axis = 1)   \n",
        "            else:\n",
        "                grow = 0\n",
        "                node[kp] = K\n",
        "                \n",
        "            #Network variance calculation\n",
        "            var = Ez2 - Ez**2\n",
        "            NHS = np.linalg.norm(var, 'fro')\n",
        "            \n",
        "            # Incremental calculation of NHS mean and variance\n",
        "            [miu_NHS,std_NHS,var_NHS] = meanstditer(miu_NHS_old,var_NHS_old,NHS,kp)\n",
        "            miu_NHS_old = miu_NHS\n",
        "            var_NHS_old = var_NHS\n",
        "            miustd_NHS  = miu_NHS + std_NHS\n",
        "            miuNHS[k-1] = miu_NHS\n",
        "\n",
        "            if(kl <= I+1 or prune ==1):\n",
        "                miumin_NHS = miu_NHS\n",
        "                stdmin_NHS = std_NHS\n",
        "            else:\n",
        "                if(miu_NHS < miumin_NHS):\n",
        "                    miumin_NHS = miu_NHS\n",
        "                if(std_NHS < stdmin_NHS):\n",
        "                    stdmin_NHS = std_NHS\n",
        "            miustdmin_NHS  = miumin_NHS + (2.6*np.exp(-NHS)+1.4)*stdmin_NHS\n",
        "            VAR[kl] = miu_NHS\n",
        "            \n",
        "            # Pruning hidden unit\n",
        "            if(grow == 0 and K>1 and miustd_NHS >= miustdmin_NHS and kl>I+1):\n",
        "                HS = Ey[1:]\n",
        "                BB = np.argmin(HS)\n",
        "                print('The node no {} is PRUNED around sample {}'.format(BB+1, k))\n",
        "                prune = 1\n",
        "                K = K-1\n",
        "                node[kp] = K\n",
        "                net.W[1] = np.delete(net.W[1],BB,axis =0)\n",
        "                net.vW[1] = np.delete(net.vW[1],BB,axis =0)\n",
        "                net.dW[1] = np.delete(net.dW[1],BB,axis =0)\n",
        "\n",
        "                net.W[2] = np.delete(net.W[2],BB+1,axis =1)\n",
        "                net.vW[2] = np.delete(net.vW[2],BB+1,axis =1)\n",
        "                net.dW[2] = np.delete(net.dW[2],BB+1,axis =1)          \n",
        "            else:\n",
        "                prune = 0\n",
        "                node[kp] = K\n",
        "                \n",
        "            #feedforward\n",
        "            net = netffsingle(net, x[k-1],y[k-1])\n",
        "        \n",
        "        #feedforward #2, executed if there is a hidden node changing      \n",
        "        net = netbackpropagation(net)\n",
        "        net = netupdate(net)\n",
        "        \n",
        "     # subsitutte weight back to main model    \n",
        "    parameter.nn.W[ly] = net.W[1]\n",
        "    parameter.nn.Ws[ly] = net.W[2]\n",
        "    \n",
        "    #reset momentum and grandient\n",
        "    parameter.nn.vW[ly] = net.vW[1] * 0\n",
        "    parameter.nn.dW[ly] = net.dW[1] * 0\n",
        "    parameter.nn.vWs[ly] = net.vW[2] * 0\n",
        "    parameter.nn.dWs[ly] = net.dW[2] *0\n",
        "    \n",
        "    #substitute the recursive calculation\n",
        "    parameter.ev[ly]['kl']          = kl\n",
        "    parameter.ev[ly]['K']           = K\n",
        "    parameter.ev[ly]['node']        = node\n",
        "    parameter.ev[ly]['BIAS2']       = BIAS2\n",
        "    parameter.ev[ly]['VAR']         = VAR\n",
        "    parameter.ev[ly]['miu_NS_old']  = miu_NS_old\n",
        "    parameter.ev[ly]['var_NS_old']  = var_NS_old\n",
        "    parameter.ev[ly]['miu_NHS_old'] = miu_NHS_old\n",
        "    parameter.ev[ly]['var_NHS_old'] = var_NHS_old\n",
        "    parameter.ev[ly]['miumin_NS']   = miumin_NS\n",
        "    parameter.ev[ly]['miumin_NHS']  = miumin_NHS\n",
        "    parameter.ev[ly]['stdmin_NS']   = stdmin_NS\n",
        "    parameter.ev[ly]['stdmin_NHS']  = stdmin_NHS                    \n",
        "    return parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjTvzSoks4jK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nettrainparallel(parameter,y):\n",
        "    bb = parameter.nn.W[parameter.nn.index].shape[1]\n",
        "    grow = 0\n",
        "    prune=0\n",
        "    #initialize performance matrix\n",
        "    ly          = parameter.nn.index\n",
        "    kp          = parameter.ev[1]['kp']\n",
        "    miu_x_old   = parameter.ev[1]['miu_x_old']\n",
        "    var_x_old   = parameter.ev[1]['var_x_old']\n",
        "    kl          = parameter.ev[ly]['kl']\n",
        "    K           = parameter.ev[ly]['K']\n",
        "    node        = parameter.ev[ly]['node']\n",
        "    BIAS2       = parameter.ev[ly]['BIAS2']\n",
        "    VAR         = parameter.ev[ly]['VAR']\n",
        "    miu_NS_old  = parameter.ev[ly]['miu_NS_old']\n",
        "    var_NS_old  = parameter.ev[ly]['var_NS_old']\n",
        "    miu_NHS_old = parameter.ev[ly]['miu_NHS_old']\n",
        "    var_NHS_old = parameter.ev[ly]['var_NHS_old']\n",
        "    miumin_NS   = parameter.ev[ly]['miumin_NS']\n",
        "    miumin_NHS  = parameter.ev[ly]['miumin_NHS']\n",
        "    stdmin_NS   = parameter.ev[ly]['stdmin_NS']\n",
        "    stdmin_NHS  = parameter.ev[ly]['stdmin_NHS']\n",
        "    \n",
        "    \n",
        "    #initiate training model\n",
        "    net = netconfigtrain([1,1,1])\n",
        "    #substitute the weight to be trained to training model\n",
        "    net.activation_func = parameter.nn.activation_func\n",
        "    net.W[1]  = parameter.nn.W[ly]\n",
        "    net.vW[1] = parameter.nn.vW[ly]\n",
        "    net.dW[1] = parameter.nn.dW[ly]\n",
        "    net.W[2]  = parameter.nn.Ws[ly]\n",
        "    net.vW[2] = parameter.nn.vWs[ly]\n",
        "    net.dW[2] = parameter.nn.dWs[ly]\n",
        "    \n",
        "    #load data in shuffled\n",
        "    x = parameter.nn.a[ly]\n",
        "    (N,I) = x.shape\n",
        "    kk = np.random.permutation(N)\n",
        "    x = x[kk]\n",
        "    y = y[kk]\n",
        "    \n",
        "    #xavier initialization\n",
        "    if(ly>1):\n",
        "        n_in = parameter.ev[ly-1]['K']\n",
        "    else:\n",
        "        n_in = parameter.nn.size[0]\n",
        "\n",
        "    miuNS       = np.zeros((N,1))\n",
        "    miuminNS       = np.zeros((N,1))\n",
        "    miuNHS       = np.zeros((N,1))\n",
        "    miuminNHS       = np.zeros((N,1))\n",
        "\n",
        "    \n",
        "    #main loop, train the model\n",
        "    for k in range(1, N+1):\n",
        "        kp = kp+1\n",
        "        kl = kl+1\n",
        "        #incremental calculation of x_tail mean and variance\n",
        "        [miu_x, std_x, var_x] = meanstditer(miu_x_old,var_x_old,parameter.nn.a[1][k-1],kp)\n",
        "        miu_x_old = miu_x\n",
        "        var_x_old = var_x\n",
        "\n",
        "        #expectation of z\n",
        "        py = probit(miu_x,std_x).T\n",
        "      \n",
        "        for ii in range(1, parameter.nn.index+1):\n",
        "            if(ii == parameter.nn.index):\n",
        "                py = sigmoid_array(np.matmul(net.W[1],py))\n",
        "            else:\n",
        "                py = sigmoid_array(np.matmul(parameter.nn.W[ii],py))\n",
        "            py = np.append(np.array([[1]]),py, axis = 0)\n",
        "            if(ii == 1):\n",
        "                Ey2 = py**2\n",
        "\n",
        "        Ey = py\n",
        "        Ez = np.matmul(net.W[2], Ey) \n",
        "        Ez = softmax_array(Ez.T).T\n",
        "        \n",
        "        \n",
        "        if(parameter.nn.hl>1):\n",
        "            py = Ey2\n",
        "            for ii in range(2, parameter.nn.index+1):\n",
        "                if(ii==parameter.nn.index):\n",
        "                    py = sigmoid_array(np.matmul(net.W[1],py))\n",
        "                else:\n",
        "                    py = sigmoid_array(np.matmul(parameter.nn.W[ii],py))\n",
        "                py = np.append(np.array([[1]]),py, axis = 0)\n",
        "            Ey2 = py\n",
        "        Ez2 = np.matmul(net.W[2], Ey2) \n",
        "        Ez2 = softmax_array(Ez2.T).T\n",
        "          \n",
        "            \n",
        "        #Network mean calculation  \n",
        "        bias2 = (Ez - np.array([[1,0]]).T)**2\n",
        "        ns = bias2\n",
        "        NS = np.linalg.norm(ns, 'fro')\n",
        "\n",
        "        #Incremental calculation of NS mean and variance\n",
        "        [miu_NS,std_NS,var_NS] = meanstditer(miu_NS_old,var_NS_old,NS,kp)\n",
        "        miu_NS_old = miu_NS\n",
        "        var_NS_old = var_NS\n",
        "        miustd_NS  = miu_NS + std_NS\n",
        "        miuNS[k-1] = miu_NS\n",
        "        \n",
        "        if(kl <= 1 or grow ==1):\n",
        "            miumin_NS = miu_NS\n",
        "            stdmin_NS = std_NS\n",
        "        else:\n",
        "            if(miu_NS < miumin_NS):\n",
        "                miumin_NS = miu_NS\n",
        "                stdmin_NS = std_NS\n",
        "            if(std_NS < stdmin_NS):\n",
        "                stdmin_NS = std_NS\n",
        "        miuminNS[k-1] = miumin_NS\n",
        "        miustdmin_NS  = miumin_NS + (1.3*np.exp(-NS)+0.7)*stdmin_NS\n",
        "        BIAS2[kl] = miu_NS\n",
        "        \n",
        "        #growing hidden unit\n",
        "        if(miustd_NS >= miustdmin_NS and kl>1):\n",
        "            grow = 1\n",
        "            K=K+1\n",
        "            print('The new node no {} is FORMED around sample {}'.format(K, k))\n",
        "            node[kp] = K\n",
        "            W_app = np.random.normal(0, np.sqrt(2/(n_in+1)), size=(1, bb))\n",
        "            net.W[1] = np.append(net.W[1],W_app, axis = 0)          \n",
        "            net.vW[1] = np.append(net.vW[1],np.zeros((1,bb)), axis = 0)\n",
        "            net.dW[1] = np.append(net.dW[1],np.zeros((1,bb)), axis = 0)\n",
        "            \n",
        "            W2_app = np.random.normal(0, np.sqrt(2/(K+1)), size=(parameter.nn.size[-1], 1))\n",
        "            net.W[2] = np.append(net.W[2],W2_app, axis = 1)\n",
        "            net.vW[2] = np.append(net.vW[2],np.zeros((parameter.nn.size[-1], 1)), axis = 1)\n",
        "            net.dW[2] = np.append(net.dW[2],np.zeros((parameter.nn.size[-1], 1)), axis = 1)\n",
        "            \n",
        "            if(ly<parameter.nn.hl):\n",
        "                wNext = parameter.nn.W[ly+1].shape[0]\n",
        "                parameter.nn.W[ly+1]  = np.append(parameter.nn.W[ly+1],np.random.normal(0, np.sqrt(2/(K+1)), size=(wNext, 1)), axis = 1)\n",
        "                parameter.nn.vW[ly+1] = np.append(parameter.nn.vW[ly+1],np.zeros((wNext, 1)), axis = 1) \n",
        "                parameter.nn.dW[ly+1] = np.append(parameter.nn.dW[ly+1],np.zeros((wNext, 1)), axis = 1)         \n",
        "        else:\n",
        "            grow = 0\n",
        "            node[kp] = K\n",
        "        #Network variance calculation\n",
        "#         ho = np.array([[0.7498],[0.2502]])\n",
        "#         he = np.array([[0.7856],[0.2144]])\n",
        "#         uy = ho-he**2\n",
        "#         ey = np.linalg.norm(uy, 'fro')\n",
        "       \n",
        "        var = Ez2 - Ez**2\n",
        "        NHS = np.linalg.norm(var, 'fro')\n",
        "        \n",
        "        # Incremental calculation of NHS mean and variance\n",
        "        [miu_NHS,std_NHS,var_NHS] = meanstditer(miu_NHS_old,var_NHS_old,NHS,kp)\n",
        "        miu_NHS_old = miu_NHS\n",
        "        var_NHS_old = var_NHS\n",
        "        miustd_NHS  = miu_NHS + std_NHS\n",
        "        miuNHS[k-1] = miu_NHS\n",
        "        \n",
        "        if(kl <= I+1 or prune ==1):\n",
        "            miumin_NHS = miu_NHS\n",
        "            stdmin_NHS = std_NHS\n",
        "        else:\n",
        "            if(miu_NHS < miumin_NHS):\n",
        "                miumin_NHS = miu_NHS\n",
        "            if(std_NHS < stdmin_NHS):\n",
        "                stdmin_NHS = std_NHS\n",
        "        miuminNHS[k-1] = miumin_NHS\n",
        "        miustdmin_NHS  = miumin_NHS + (2.6*np.exp(-NHS)+1.4)*stdmin_NHS\n",
        "        VAR[kl] = miu_NHS\n",
        "        \n",
        "        # Pruning hidden unit\n",
        "        if(grow == 0 and K>1 and miustd_NHS >= miustdmin_NHS and kl>I+1):\n",
        "            HS = Ey[1:]\n",
        "            BB = np.argmin(HS)\n",
        "            print('The node no {} is PRUNED around sample {}'.format(BB+1, k))\n",
        "            prune = 1\n",
        "            K = K-1\n",
        "            node[kp] = K\n",
        "            net.W[1] = np.delete(net.W[1],BB,axis =0)\n",
        "            net.vW[1] = np.delete(net.vW[1],BB,axis =0)\n",
        "            net.dW[1] = np.delete(net.dW[1],BB,axis =0)\n",
        "            \n",
        "            net.W[2] = np.delete(net.W[2],BB+1,axis =1)\n",
        "            net.vW[2] = np.delete(net.vW[2],BB+1,axis =1)\n",
        "            net.dW[2] = np.delete(net.dW[2],BB+1,axis =1)\n",
        "            if(ly<parameter.nn.hl):\n",
        "                parameter.nn.W[ly+1]  = np.delete(parameter.nn.W[ly+1],BB+1,axis =1)\n",
        "                parameter.nn.vW[ly+1] = np.delete(parameter.nn.vW[ly+1],BB+1,axis =1)\n",
        "                parameter.nn.dW[ly+1] = np.delete(parameter.nn.dW[ly+1],BB+1,axis =1)            \n",
        "        else:\n",
        "            prune = 0\n",
        "            node[kp] = K\n",
        "        #feedforward\n",
        "        net = netffsingle(net, x[k-1],y[k-1])\n",
        "        \n",
        "        #feedforward #2, executed if there is a hidden node changing      \n",
        "        net = netbackpropagation(net)\n",
        "        net = netupdate(net)\n",
        "        \n",
        "    # subsitutte weight back to main model    \n",
        "    parameter.nn.W[ly] = net.W[1]\n",
        "    parameter.nn.Ws[ly] = net.W[2]\n",
        "    \n",
        "    #reset momentum and grandient\n",
        "    parameter.nn.vW[ly] = net.vW[1] * 0\n",
        "    parameter.nn.dW[ly] = net.dW[1] * 0\n",
        "    parameter.nn.vWs[ly] = net.vW[2] * 0\n",
        "    parameter.nn.dWs[ly] = net.dW[2] *0\n",
        "    \n",
        "    #substitute the recursive calculation\n",
        "    parameter.ev[1]['kp']           = kp\n",
        "    parameter.ev[1]['miu_x_old']    = miu_x_old\n",
        "    parameter.ev[1]['var_x_old']    = var_x_old\n",
        "    parameter.ev[ly]['kl']          = kl\n",
        "    parameter.ev[ly]['K']           = K\n",
        "    parameter.ev[ly]['node']        = node\n",
        "    parameter.ev[ly]['BIAS2']       = BIAS2\n",
        "    parameter.ev[ly]['VAR']         = VAR\n",
        "    parameter.ev[ly]['miu_NS_old']  = miu_NS_old\n",
        "    parameter.ev[ly]['var_NS_old']  = var_NS_old\n",
        "    parameter.ev[ly]['miu_NHS_old'] = miu_NHS_old\n",
        "    parameter.ev[ly]['var_NHS_old'] = var_NHS_old\n",
        "    parameter.ev[ly]['miumin_NS']   = miumin_NS\n",
        "    parameter.ev[ly]['miumin_NHS']  = miumin_NHS\n",
        "    parameter.ev[ly]['stdmin_NS']   = stdmin_NS\n",
        "    parameter.ev[ly]['stdmin_NHS']  = stdmin_NHS\n",
        "    #print(BIAS2)             \n",
        "    return parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKERSwn5s4jM",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za_c4ysrs4jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = scipy.io.loadmat('/home/visitor-2/Documents/Datasets/sea.mat')\n",
        "data = data1.get('data')\n",
        "n_feature = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZQwTPhs4jO",
        "colab_type": "text"
      },
      "source": [
        "# Main Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD19e9-Bs4jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_ADL(data, n_feature):   \n",
        "    (nData, n_column) = data.shape\n",
        "    M = n_column - n_feature\n",
        "    preq_data = data[:,0:n_feature]\n",
        "    preq_label = data[:,n_feature:]  \n",
        "    chunk_size = 500\n",
        "    no_of_chunk = int(nData/chunk_size)\n",
        "    \n",
        "    drift = {}\n",
        "    HL = {}\n",
        "    buffer_x = []\n",
        "    buffer_T = []\n",
        "    tTest = []\n",
        "    tTarget = []\n",
        "    act = []\n",
        "    out = []\n",
        "    #initiate model \n",
        "    K = 1 #initial node\n",
        "    network = nn([n_feature, K, M])\n",
        "    \n",
        "    #initiate node evolving iterative parameters\n",
        "    layer = 1 #initial layer\n",
        "    parameter = Parameter(network, layer,K)\n",
        "    performance = Performance()\n",
        "    \n",
        "    # initiate drift detection parameter\n",
        "    alpha_w = 0.0005;\n",
        "    alpha_d = 0.0001;\n",
        "    alpha   = 0.0001;\n",
        "    \n",
        "    #initiate layer merging iterative parameters\n",
        "    covariance = np.zeros((1,1,2))\n",
        "    covariance_old             = covariance\n",
        "    threshold                  = 0.05\n",
        "    \n",
        "    ClassificationRate = {}\n",
        "    for count in range(0,no_of_chunk):  \n",
        "        # prepare data\n",
        "        n = count + 1\n",
        "        minibatch_data  = preq_data [(n-1)*chunk_size:n*chunk_size]\n",
        "        minibatch_label = preq_label[(n-1)*chunk_size:n*chunk_size]\n",
        "        \n",
        "        # neural network testing      \n",
        "        print('Chunk: {} of {}'.format(n, no_of_chunk))\n",
        "        print('Discriminative Testing: running ...')\n",
        "        parameter.nn.t = n\n",
        "        [parameter.nn] = nettestparallel(parameter.nn,minibatch_data,minibatch_label,parameter.ev)\n",
        "        \n",
        "        #metrics calculation\n",
        "        parameter.Loss[n] = parameter.nn.L[parameter.nn.index]\n",
        "        if(n == 1):\n",
        "            tTest = parameter.nn.sigma.copy()\n",
        "            act = parameter.nn.act.copy()\n",
        "            out = parameter.nn.out.copy()\n",
        "            parameter.residual_error = np.append(out,parameter.nn.residual_error,axis=0)\n",
        "        else:\n",
        "            tTest = np.append(tTest,parameter.nn.sigma,axis=0)\n",
        "            act = np.append(act,parameter.nn.act,axis=0)\n",
        "            out = np.append(out,parameter.nn.out,axis=0)\n",
        "            parameter.residual_error = np.append(out,parameter.nn.residual_error,axis=0)\n",
        "        parameter.cr[n] = parameter.nn.cr;\n",
        "        ClassificationRate[n] = np.array(list(parameter.cr.values())).mean()\n",
        "        print('Classification rate {}'.format(ClassificationRate[n]))\n",
        "        print('Discriminative Testing: ... finished')\n",
        "        \n",
        "        #statistical measure\n",
        "        performance.ev[n] = {}\n",
        "        [performance.ev[n]['f_measure'], performance.ev[n]['g_mean'] ,performance.ev[n]['recall'],performance.ev[n]['precision']] = performance_summary(parameter.nn.act, parameter.nn.out, M)\n",
        "        #last chunk only for testing process\n",
        "        if(n == no_of_chunk):\n",
        "            print('=========Parallel Autonomous Deep Learning is finished=========')\n",
        "            break\n",
        "\n",
        "        #Drift detection: output space\n",
        "        if(n>1):\n",
        "            cuttingpoint = 0\n",
        "            pp = minibatch_label.shape[0]\n",
        "            F_cut = np.zeros((pp,1))\n",
        "            F_cut[parameter.nn.bad] = 1\n",
        "            Fupper = np.max(F_cut)\n",
        "            Flower = np.min(F_cut)\n",
        "            miu_F = np.mean(F_cut)\n",
        "            \n",
        "            for idx in range(pp):\n",
        "                cut = idx + 1\n",
        "                miu_G = np.mean(F_cut[0:cut])\n",
        "                Gupper = np.max(F_cut[0:cut])\n",
        "                Glower = np.min(F_cut[0:cut])\n",
        "                epsilon_G = (Gupper - Glower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))\n",
        "                epsilon_F = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha)))\n",
        "                if ((epsilon_G + miu_G) >= (miu_F + epsilon_F) and cut<pp):\n",
        "                    cuttingpoint = cut\n",
        "                    miu_H = np.mean(F_cut[(cuttingpoint):])\n",
        "                    epsilon_D = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_d)))\n",
        "                    epsilon_W = (Fupper - Flower) * np.sqrt(((pp-cuttingpoint)/(2*cuttingpoint*(pp-cuttingpoint)) * np.log(1/alpha_w)))\n",
        "                    break\n",
        "            if(cuttingpoint == 0):\n",
        "                miu_H = miu_F\n",
        "                epsilon_D = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha_d)))\n",
        "                epsilon_W = (Fupper - Flower) * np.sqrt(((pp)/(2*cut*(pp)) * np.log(1/alpha_w)))\n",
        "            \n",
        "            #DRIFT STATUS\n",
        "            if((np.abs(miu_G - miu_H)) > epsilon_D and cuttingpoint>1):\n",
        "                st = 1\n",
        "                print('Drift state: DRIFT')\n",
        "                layer = layer+1\n",
        "                parameter.nn.n = parameter.nn.n + 1\n",
        "                parameter.nn.hl = layer\n",
        "                print('The new Layer no {} is FORMED around chunk {}'.format(layer, n))\n",
        "                \n",
        "                #Initiate NN weight parameters\n",
        "                ii = parameter.nn.W[layer-1].shape[0]\n",
        "                parameter.nn.W[layer] = np.random.normal(0,np.sqrt(2/(ii+1)),size = (1, (ii+1)))    \n",
        "                parameter.nn.vW[layer] = np.zeros((1,ii+1))\n",
        "                parameter.nn.dW[layer] = np.zeros((1,ii+1))\n",
        "                \n",
        "                #Initiate new classifier weight\n",
        "                parameter.nn.Ws[layer]  = np.random.normal(0,1,size = (M,2))    \n",
        "                parameter.nn.vWs[layer] = np.zeros((M,2))\n",
        "                parameter.nn.dWs[layer] = np.zeros((M,2))\n",
        "                \n",
        "                #Initiate new voting weight\n",
        "                parameter.nn.beta[layer] = 1\n",
        "                parameter.nn.betaOld[layer] = 1\n",
        "                parameter.nn.p[layer] = 1\n",
        "                \n",
        "                # Initiate iterative parameters\n",
        "                parameter.ev[layer] = {}\n",
        "                parameter.ev[layer]['layer ']      = layer\n",
        "                parameter.ev[layer]['kl']          = 0\n",
        "                parameter.ev[layer]['K']           = 1\n",
        "                parameter.ev[layer]['cr']           = 0\n",
        "                parameter.ev[layer]['node']        = {}\n",
        "                parameter.ev[layer]['miu_NS_old']  = 0\n",
        "                parameter.ev[layer]['var_NS_old']  = 0\n",
        "                parameter.ev[layer]['miu_NHS_old'] = 0\n",
        "                parameter.ev[layer]['var_NHS_old'] = 0\n",
        "                parameter.ev[layer]['miumin_NS']   = []\n",
        "                parameter.ev[layer]['miumin_NHS']  = []\n",
        "                parameter.ev[layer]['stdmin_NS']   = []\n",
        "                parameter.ev[layer]['stdmin_NHS']  = []\n",
        "                parameter.ev[layer]['BIAS2']       = {}\n",
        "                parameter.ev[layer]['VAR']         = {} \n",
        "                \n",
        "                #check buffer\n",
        "                if(len(buffer_x) == 0):\n",
        "                    h = parameter.nn.a[len(parameter.nn.a)][:,1:]\n",
        "                    z = minibatch_label\n",
        "                else:\n",
        "                    buffer_x = netffhl(parameter.nn, buffer_x)\n",
        "                    h = np.append(buffer_x[:,1:],parameter.nn.a[len(parameter.nn.a)][:,1:],axis=0)\n",
        "                    if(len(buffer_T) == 0):\n",
        "                        z = np.append(buffer_T,minibatch_label ,axis=0)\n",
        "                    else:\n",
        "                        z = minibatch_label\n",
        "                        \n",
        "                #Discriminative training for new layer\n",
        "                print('Discriminative Training for new layer: running ...')\n",
        "                parameter = nettrainsingle(parameter,h,z)\n",
        "                print('Discriminative Training for new layer: ... finished')\n",
        "                buffer_x = []\n",
        "                buffer_T = []\n",
        "            elif((np.abs(miu_G - miu_H)) >= epsilon_W and (np.abs(miu_G - miu_H)) < epsilon_D):\n",
        "                st = 2\n",
        "                print('Drift state: WARNING')\n",
        "                buffer_x = minibatch_data\n",
        "                buffer_T = minibatch_label\n",
        "            else:\n",
        "                st = 3\n",
        "                print('Drift state: STABLE')\n",
        "                buffer_x = []\n",
        "                buffer_T = []\n",
        "        else:\n",
        "            st = 3\n",
        "            print('Drift state: STABLE')\n",
        "            buffer_x = []\n",
        "            buffer_T = []\n",
        "        drift[n] = st\n",
        "        HL[n] = checkbeta(parameter.nn.beta)\n",
        "        parameter.wl[n] = parameter.nn.index\n",
        "        \n",
        "        #Discriminative training for winning layer\n",
        "        if(st != 1):\n",
        "            print('Discriminative Training: running ...')\n",
        "            parameter = nettrainparallel(parameter, minibatch_label)\n",
        "            print('Discriminative Training: ... finished')\n",
        "            \n",
        "        #Clear current data chunk\n",
        "        parameter.nn.a = {}\n",
        "        print('=========Hidden layer number {} was updated========='.format(parameter.nn.index))\n",
        "    return parameter,performance    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXtKmkpns4jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parameter, performance = test_ADL(data,n_feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p6YQGNBs4jT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiNcLmcqs4jV",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgOznxCs4jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#method for precission, recall, f1 score and gmean\n",
        "def performance_summary(act_raw, out_raw, nclass):\n",
        "    Act = to_one_hot(act_raw, nclass)\n",
        "    Out = to_one_hot(out_raw, nclass)\n",
        "    recall = compute_recall(Act, Out, nclass)\n",
        "    precission = compute_precission(Act, Out, nclass)\n",
        "    f_measure = compute_f_measure(Act, Out, nclass)\n",
        "    g_mean = compute_g_mean(recall, nclass)\n",
        "    return [g_mean, f_measure, precission, recall]\n",
        "\n",
        "def compute_g_mean(recall, nclass):\n",
        "    g_mean = (np.prod(recall))**(1/nclass) \n",
        "    return g_mean\n",
        "\n",
        "def compute_f_measure(Act, Out, nclass):\n",
        "    f_measure = np.zeros((1,nclass))    \n",
        "    for c in range(nclass):\n",
        "        f_measure[0][c] = (2 * (Act[:,c].T.dot(Out[:,c]))) / (np.sum(Out[:,c]) + np.sum(Act[:,c]))\n",
        "    f_measure[np.isnan(f_measure)] = 1\n",
        "    return f_measure\n",
        "\n",
        "def compute_precission(Act, Out, nclass):\n",
        "    precission = np.zeros((1,nclass))    \n",
        "    for c in range(nclass):\n",
        "        precission[0][c] = (Act[:,c].T.dot(Out[:,c])) / np.sum(Out[:,c])\n",
        "    precission[np.isnan(precission)] = 1\n",
        "    return precission\n",
        "    \n",
        "def compute_recall(Act, Out, nclass):\n",
        "    recall = np.zeros((1,nclass))  \n",
        "    for c in range(nclass):\n",
        "        recall[0][c] = (Act[:,c].T.dot(Out[:,c])) / np.sum(Act[:,c])\n",
        "    recall[np.isnan(recall)] = 1\n",
        "    return recall\n",
        "\n",
        "def to_one_hot(x, nclass):\n",
        "    y = np.zeros((x.shape[0],nclass))\n",
        "    for i in range(len(x)):\n",
        "        y[i][x[i]] = 1\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "G7rj9rtDs4jY",
        "colab_type": "code",
        "colab": {},
        "outputId": "173d65f9-1934-42da-96af-114196865111"
      },
      "source": [
        "[g,f,p,r] = performance_summary(act1[1:10],out1[1:10],2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/visitor-2/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DL5wUgPBs4jc",
        "colab_type": "code",
        "colab": {},
        "outputId": "8bcc2189-efd4-4047-fc21-f06a48448c3c"
      },
      "source": [
        "(g, f, p ,r)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0,\n",
              " array([[0.71428571, 0.        ]]),\n",
              " array([[0.55555556, 1.        ]]),\n",
              " array([[1., 0.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OkZS0M3s4je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}